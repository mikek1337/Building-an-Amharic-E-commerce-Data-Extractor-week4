{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7db9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('../scripts')\n",
    "from preprocess import process_text, process_language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d037d4",
   "metadata": {},
   "source": [
    "### Inital cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76fda50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/messages.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03459c89",
   "metadata": {},
   "source": [
    "**removes missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b85dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f2a43",
   "metadata": {},
   "source": [
    "**Removes emojes, tabs and other characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74f8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['message'] = data['message'].apply(process_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5f149",
   "metadata": {},
   "source": [
    "**Load cleaned messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5d8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/cleaned_messages.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd70fe",
   "metadata": {},
   "source": [
    "**Save tokenized messages to a message_tokens.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ff08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imitation', 'volcano', 'humidifier', 'with', 'led', 'light', 'በኤሌክትሪክ', ':', 'የሚሰራ', 'ለቤት', 'መልካም', 'መአዛን', 'የሚሰጥ', 'ዋጋ፦', '1400', 'ብር', 'ውስን', 'ፍሬ', 'ያለን', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['baby', 'carrier', 'በፈለጉት', 'አቅጣጫ', 'ልጅዎን', 'በምቾት', 'ማዘል', 'ያስችልዎታል', 'ዋጋ፦', '2400', 'ብር', 'ውስን', 'ፍሬ', 'ያለን', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['smart', 'usb', 'ultrasonic', 'car', 'and', 'home', 'air', 'humidifier', 'with', 'colorful', 'led', 'light', 'original', 'high-quality', 'በኤሌክትሪክ', 'የሚሰራ', 'ለቤትና', 'ለመኪና', 'መልካም', 'መአዛን', 'የሚሰጥ', 'elevate', 'the', 'comfort', 'level', 'within', 'your', 'living', 'premises', 'with', 'this', 'fantastic', 'green', 'lion', 'air', 'mist', 'humidifier', 'ዋጋ፦', '1100', 'ብር', 'ውስን', 'ፍሬ', 'ያለን', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['baby', 'head', 'helmet', 'cotton', 'walk', 'safety', 'hat', 'breathable', 'headgear', 'toddler', 'antifall', 'pad', 'this', 'product', 'is', 'made', 'of', 'non-toxic', 'cotton', 'material', 'it', 'can', 'be', 'adjustable', 'to', 'fits', 'all', 'the', 'different', 'size', 'of', 'head', 'the', 'product', 'is', 'also', 'breathable', 'and', 'the', 'baby', \"'s\", 'head', 'will', 'not', 'be', 'sultry', 'and', 'muggy', 'when', 'playing', 'ዋጋ፦', '550', 'ብር', 'ውስን', 'ፍሬ', 'ያለን', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['በረፍት', 'ቀንዎ', 'ሱቅ', 'መስተናገድ', 'ለምትፈልጉ', 'ውድ', 'ደንበኞቻችን', 'ነገ', 'ከጠዋቱ', '4:30', '_', 'ቀኑ', '11:00', 'ድረስ', 'ሱቃችን', 'ክፍት', 'ሁኖ', 'ይጠብቅዎታል', 'ሱቅ', 'መተው', 'መግዛት', 'ላልቻላችሁ', 'በሞተረኞች', 'ያሉበት', 'እናደርሳለን', 'ዘወትር', 'ሰኞ', '_ቅዳሜ', 'ከጠዋቱ', '2:30', 'ምሽቱ', '2:00', 'ድረስ', 'ክፍት', 'እንገልፃለን', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['baby', 'knee', 'socks', 'ልጅዎ', 'መዳህ', 'በሚጀምሩበት', 'ተመራጭ', 'ዋጋ፦', '250', 'ብር', 'ውስን', 'ፍሬ', 'ያለን', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['5-in-1', 'trouser', 'hanger', 'የሱሪ', 'ማስቀመጫ', 'ዋጋ፦', '650', 'ብር', 'ውስን', 'ፍሬ', 'ያለን', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['waterproof', 'shower', 'cap', 'reusable', 'ዋጋ፦', '200', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['shock', 'and', 'noise', 'cancelling', 'washing', 'machine', 'support', 'washer', 'and', 'dryer', 'pedestals', ',', 'anti', 'slip', 'anti', 'vibration', 'rubber', 'washing', 'machine', 'feet', 'pads', 'washing', 'machine', 'stabilizer', 'for', 'raise', 'height', 'reduce', 'noise', '4pcs', 'ዋጋ፦', '500', 'ብር', 'ውስን', 'ፍሬ', 'ያለን', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['magic', 'silicone', 'dish', 'washing', 'gloves', 'high', 'quality', '(', 'ወፍራሙ', ')', 'እቃ', 'ለማጠብ', 'ቤት', 'ለማፅዳት', 'መኪና', 'ለማጠብ', 'ለተለያየ', 'አገልግሎት', 'የሚውል', 'ዋጋ፦', '350ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['13pc', 'portable', 'health', 'care', 'kit', 'designed', 'for', 'newborn', 'babies', '13', 'pieces', 'of', 'baby', 'care', 'set', ',', 'stored', 'in', 'a', 'carton', 'for', 'easy', 'storage', '&', 'organizing', 'simple', ',', 'convenientandeasytouse', 'madeofgreenmaterialstocareforthebaby', '’', 'sgrowth', 'effectivelyprotectbaby', '’', 'shealth', 'newandhighquality', 'ዋጋ፦', '1100', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['three-layer', 'baby', 'milk', 'powder', 'container', 'high', 'quality', 'three', 'layer', 'no-spill', 'baby', 'feeding', 'milk', 'powder', 'food', 'dispenser', 'a', 'perfect', 'storage', 'for', 'travel', 'or', 'home', 'use', 'እናት', 'ልጇን', 'ይዛ', 'የተለያየ', 'ቦታ', 'ስትንቀሳቀስ', 'የዱቄት', 'ወተት', 'የመሳሰሉትን', 'አስፈላጊ', 'የልጆች', 'ምግብ', 'ይዞ', 'ለመንቀሳቀስ', 'የሚረዳ', '3', 'ፓርቲሽን', 'አሪፍ', 'ኮንቴነር', 'ዋጋ፦', '550ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['three-layer', 'baby', 'milk', 'powder', 'container', 'high', 'quality', 'three', 'layer', 'no-spill', 'baby', 'feeding', 'milk', 'powder', 'food', 'dispenser', 'a', 'perfect', 'storage', 'for', 'travel', 'or', 'home', 'use', 'እናት', 'ልጇን', 'ይዛ', 'የተለያየ', 'ቦታ', 'ስትንቀሳቀስ', 'የዱቄት', 'ወተት', 'የመሳሰሉትን', 'አስፈላጊ', 'የልጆች', 'ምግብ', 'ይዞ', 'ለመንቀሳቀስ', 'የሚረዳ', '3', 'ፓርቲሽን', 'አሪፍ', 'ኮንቴነር', 'ዋጋ፦', '500ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['አልቋል', 'ለተባላችሁ', 'በድጋሜ', 'ገብቷል', 'only', 'baby', '3in1', 'double', 'bottle', 'milk', 'warmer', ',', 'sterilizer', ',', 'food', 'steamer', 'ለህፃን', 'ወተት', 'ማሞቂያ', 'በተጨማሪ', 'ምግብ', 'ለመቀቀል', 'የሚሆን', 'ዋጋ፦', '3000', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['mini', 'pocket', 'uv', 'umbrella', 'የቀለም', 'አማራጭ', 'አላቸው', 'ቀላል፣', 'ለመያዝ', 'ምቹ', 'ጥላ', 'በትንሽ', 'የእጅ', 'ቦርሳ', 'በኪስ', 'መያዝ', 'የሚችል', ':', 'compact', '&', 'light-weight', 'unique', 'design', 'uv', 'protection', 'ዋጋ፦', '1000ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['waterproof', 'baby', 'urine', 'mat', 'cover', 'ውሀ', 'የማያስገባ', 'እንደገና', 'ሊታጠብ', 'የሚችል', 'የህፃን', 'የሽንት', 'ምንጣፍ።', 'በተጨማሪም', 'ዳይፐር', 'ለመቀየር', 'የሚያገለግል', 'reusable', 'washable', 'size', ':', '1m×80cm', 'ዋጋ፦', '650ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['space', 'triangles', 'for', 'hangers', '12pcs', 'ዋጋ፦', '400ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['door', 'bottom', 'seal', 'strip', 'stopper', 'በበርዎ', 'ስር', 'አቧራ', 'ነፍሳት', 'እንዳይገባ', 'የሚከላከል', '!', '!', '!', 'high', 'quality', '®', 'አንደኛው', 'ወፍራሙ', 'keep', 'hot', 'and', 'cold', 'air', 'isolated', 'reduce', 'the', 'noise', 'outside', 'and', 'door', 'slamming', 'no', 'ዋጋ፦', '400ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['one', 'fire', 'table', 'lamp', 'በተች', 'የሚሰራ', 'ቻርጅ', 'የሚደረግ', 'ለመኝታ', 'ቤት', '፣', 'ለሳሎን', '፣', 'ሆቴሎች', 'ለባዝ', 'ሩም', 'ዋጋ፦', '1900ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['vaccuum', 'flask', 'set', 'የፔርሙዝ', 'ማግ', '3', 'መጠጫ', 'ኩባያዎች', 'ያሉት', '12', 'ሰአታት', 'የሞቀዉን', 'አሙቆ', 'የቀዘቀዘዉን', 'አቀዝቅዞ', 'ሚያስቀምጥ', 'ድንገት', 'ቢወድቅ', 'ከማይሰበር', 'ማቴሪያል', 'የተሰራ', '500ሚሊ', 'ሊትር', 'የመያዝ', 'አቅም', 'ያለዉ', 'በተለያየ', 'የከለር', 'አማራጭ', 'የቀረበ', 'ለስጦታ', 'የሚሆን', 'ማሸጊያና', 'መያዣ', 'ያለዉ', 'ዋጋ፦', '1200', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['ለመላው', 'የእስልምና', 'እምነት', 'ተከታዮች', 'በሙሉ', 'ለኢድ', 'አል', 'አድሀ', '(', 'አረፋ', ')', 'በአል', 'በሰላም', 'አደረሳችሁ', 'ሱቃችን', 'ሙሉ', 'ክፍት', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['stainless', 'steel', 'potato', 'masher', 'quality', 'ዋጋ፦', '450', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['2', 'in1', 'electric', 'foot', 'grinder', '&', 'hair', 'remover', 'የተረከዝ', 'ሞረድ', 'የፀጉር', 'ማንሻ', 'ቻርጅ', 'የሚሰራ', 'ዋጋ፦', '1000', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['garlic', 'press', 'chopper', 'kitchen', 'ginger', 'garlic', 'pressquick', 'grindingsaving', 'you', 'time', 'manual', 'extrusion', 'and', 'grinding', 'stainless', 'steel', 'materialfood', 'grade', 'safety', 'materialdurable', 'and', 'easy', 'to', 'clean', 'curved', 'plastic', 'handlecomfortable', 'to', 'use', 'ዋጋ፦', '350', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['over', 'toilet', 'storage', 'rack', 'የቦታ', 'ጥበት', 'ካለብዎ', 'ይሄ', 'ያስፈልግዎታል', 'toilet/kitchen', 'ሊጠቀሙበት', 'ይችላሉ', 'የንፅህና', 'እቃዎችን፣', 'ኮስሞቲክሶች', 'አትክልት/ፍራፍሬዎችን', 'ሊያስቀምጡበት', 'ይችላሉ', 'ግድግዳ', 'መብሳት', 'አያስፈልግዎትም', 'ከጠንካራ', 'ብረት', 'ፕላስቲክ', 'የተሰራ', 'እርጥበትን', 'የሚቋቋም', 'የማይዝግ', 'ማያያዣ', 'ሁክ', 'ዋጋ፦', '1500', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['coffee', 'cold', 'extraction', 'cup', 'ice', 'coffee', 'maker', 'portable', 'cold', 'brew', 'bottle', 'maker', 'mug', 'leak', 'proof', 'airtight', 'lid', 'with', 'mesh', 'filter', 'ice', 'coffee', 'jar', 'tea', 'infuser', 'for', 'outdoor', 'የአጠቃቀም', 'ዘዴ', '1⃣', ',', 'ለቡና', 'ዱቄት', 'የሚሆን', 'በቂ', 'ቦታ', 'በመተው', 'በውሀ', 'መሙላት', 'ይጀምሩ', '2⃣', ',', 'ባለው', 'ማጣሪያ', 'የሚፈልጉትን', 'የቡና', 'መጠን', 'ይጨምሩ', '3⃣', ',', 'የማጣሪያውን', 'ክዳን', 'አጥብቀው', 'ይዝጉ', 'የቡናውን', 'ቦታ', 'በእኩል', 'መጠን', 'ለማከፋፈል', 'ቀስ', 'ማወዛወዝ', '4⃣', ',', 'ለተወሰኑ', 'ሰአታት', 'ፍሪጅ', 'ማስቀመጥ', '5⃣', ',', 'ቀዝቃዛ', 'ቡናዎን', 'በበረዶ', 'ከወተት', 'በመቀላቀል', 'መጠጣት', 'ይችላሉ', 'ዋጋ፦', '900', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['floodlight', 'head', 'lamp', 'rechargeable', 'light', 'weight', 'applicable', 'to', 'various', 'scenarios', 'such', 'as', 'camping', 'night', 'running', 'repairing', 'cycling', 'ዋጋ፦', '950', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['car', 'solar', 'aromatherapy', 'ፀሀይ', 'ብርሀን', 'የሚሰራ', 'ለመኪናዎ', 'ጥሩ', 'ጠረን', 'የሚያላብስ', 'እየተሽከረከረ', 'አየር', 'በማፅዳት', 'ጥሩ', 'መአዛን', 'የሚሰጥ', 'ለመኪናዎ', 'ተጨማሪ', 'ውበትን', 'የሚሰጥ', 'በተመጣጣኝ', 'ዋጋ', 'ዋጋ፦', '850', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['silicone', 'thumb', 'cutter', 'የጣት', 'ጥፍር', 'መከላከያ', 'ሽንኩርት', 'ሲልጡ', '፣', 'ዝንጅብል', 'በርበሬ', 'ዘለላ', 'ሲቀነጥሱ', 'ጥፍርን', 'ከጉዳት', 'የሚከላከል', 'material', ':', 'stainless', 'steel', ',', 'silicone', ',', 'pp', 'colour', 'ዋጋ፦', '250', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n",
      "['portable', 'drying', 'rack', 'clips', 'cloth', 'hanger', 'ልብስ', 'ማስጫ', 'የሚዘረጋ', 'የሚሰበሰብ', 'የልብስ', 'መቆንጠጫ', '180', 'cm', 'ርዝመት', 'ተንቀሳቃሽ', 'ዋጋ፦', '500', 'ብር', 'ውስን', 'ፍሬ', 'አድራሻ', '#', 'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ', 'ቢሮ', 'ቁ', 's05/s06', '0902660722', '0928460606', 'በTelegram', 'ለማዘዝ', 'ይጠቀሙ', 'ለተጨማሪ', 'ማብራሪያ', 'የቴሌግራም', 'ገፃችን']\n"
     ]
    }
   ],
   "source": [
    "col_data = data['message'].drop_duplicates().head(30).apply(process_language).to_list()\n",
    "with open(f'../data/message_tokens.txt', 'w') as f:\n",
    "    for col in col_data:\n",
    "        tokens = col.split(' ')\n",
    "        for token in tokens:\n",
    "            if(token !=':' or token !='#'):\n",
    "                f.write(f'{token}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "602ef976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "message",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7de946f5-757f-4347-bced-f4e12b06acae",
       "rows": [
        [
         "0",
         "  Imitation Volcano Humidifier with LED Light  በኤሌክትሪክ:የሚሰራ ለቤት መልካም መዓዛን የሚሰጥ  ዋጋ፦  1400 ብር  ውስን ፍሬ ነው ያለን   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "1",
         "   Baby Carrier  በፈለጉት አቅጣጫ ልጅዎን በምቾት ማዘል ያስችልዎታል  ዋጋ፦  2400 ብር  ውስን ፍሬ ነው ያለን   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "2",
         "  Smart Usb Ultrasonic Car And Home Air Humidifier With Colorful Led Light   Original  High-quality   በኤሌክትሪክ የሚሰራ  ለቤትና ለመኪና መልካም መዓዛን የሚሰጥ  Elevate the comfort level within your living premises with this fantastic Green Lion Air Mist Humidifier  ዋጋ፦  1100 ብር  ውስን ፍሬ ነው ያለን   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "4",
         "  Baby Head Helmet Cotton Walk Safety Hat Breathable Headgear Toddler Antifall Pad  This product is made of non-toxic cotton material  It can be adjustable to fits all the different size of head  The product is also breathable and the baby's head will not be sultry and muggy when playing  ዋጋ፦  550 ብር  ውስን ፍሬ ነው ያለን   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "6",
         "   በረፍት ቀንዎ ሱቅ ላይ መስተናገድ ለምትፈልጉ ውድ ደንበኞቻችን   ነገ ከጠዋቱ 4:30 _ ቀኑ 11:00 ድረስ  ሱቃችን ክፍት ሁኖ ይጠብቅዎታል     ሱቅ መተው መግዛት ላልቻላችሁ በሞተረኞች ያሉበት እናደርሳለን    ዘወትር ሰኞ _ቅዳሜ ከጠዋቱ 2:30 እስከ ምሽቱ 2:00 ድረስ ክፍት መሆኑን እንገልፃለን   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "7",
         "   Baby knee socks  ልጅዎ መዳህ በሚጀምሩበት ጊዜ ተመራጭ  ዋጋ፦  250 ብር  ውስን ፍሬ ነው ያለን   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "9",
         "  5-in-1 Trouser Hanger የሱሪ ማስቀመጫ  ዋጋ፦  650 ብር  ውስን ፍሬ ነው ያለን   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "10",
         "  WaterProof Shower Cap    reusable  ዋጋ፦  200 ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "11",
         "   Shock and Noise Cancelling Washing Machine Support  Washer and Dryer Pedestals,Anti Slip Anti Vibration Rubber Washing Machine Feet Pads Washing Machine Stabilizer for Raise Height  Reduce Noise 4PCS  ዋጋ፦  500 ብር  ውስን ፍሬ ነው ያለን   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "14",
         "   Magic Silicone Dish Washing Gloves  High Quality (ወፍራሙ)   ዕቃ ለማጠብ ቤት ለማፅዳት መኪና ለማጠብ   ለተለያየ አገልግሎት የሚውል  ዋጋ፦  350ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "16",
         "   13pc Portable Health Care Kit  Designed For Newborn Babies   13 pieces of baby care set, stored in a carton for easy storage & organizing  Simple,convenientandeasytouse Madeofgreenmaterialstocareforthebaby’sgrowth effectivelyprotectbaby’shealth newandhighquality  ዋጋ፦  1100 ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "17",
         "   Three-layer Baby Milk Powder Container    High Quality    Three Layer No-Spill Baby Feeding Milk Powder Food Dispenser A perfect storage for travel or home use  እናት ልጇን ይዛ የተለያየ ቦታ ስትንቀሳቀስ የዱቄት ወተት የመሳሰሉትን አስፈላጊ የልጆች ምግብ ይዞ ለመንቀሳቀስ የሚረዳ 3 ፓርቲሽን ያለው አሪፍ ኮንቴነር  ዋጋ፦  550ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "18",
         "   Three-layer Baby Milk Powder Container    High Quality    Three Layer No-Spill Baby Feeding Milk Powder Food Dispenser A perfect storage for travel or home use  እናት ልጇን ይዛ የተለያየ ቦታ ስትንቀሳቀስ የዱቄት ወተት የመሳሰሉትን አስፈላጊ የልጆች ምግብ ይዞ ለመንቀሳቀስ የሚረዳ 3 ፓርቲሽን ያለው አሪፍ ኮንቴነር  ዋጋ፦  500ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "19",
         " አልቋል ለተባላችሁ በድጋሜ ገብቷል    Only baby 3in1 double bottle milk warmer,sterilizer,food steamer  ለሕፃን ወተት ማሞቂያ  በተጨማሪ ምግብ ለመቀቀል የሚሆን   ዋጋ፦ 3000 ብር  ውስን ፍሬ ነው ያለው   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "21",
         "   Mini Pocket UV Umbrella   የቀለም አማራጭ አላቸው  በጣም ቀላል፣ ለመያዝ ምቹ ጥላ  በትንሽ የእጅ ቦርሳ ወይም በኪስ መያዝ የሚችል  :  Compact & Light-Weight Unique Design UV protection  ዋጋ፦  1000ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "23",
         "  Waterproof Baby Urine Mat Cover  ውሃ የማያስገባ እንደገና ሊታጠብ የሚችል የሕፃን የሽንት ምንጣፍ።  በተጨማሪም ዳይፐር ለመቀየር የሚያገለግል  Reusable  Washable  Size: 1m×80cm  ዋጋ፦ 650ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "24",
         "  Space triangles for hangers   12pcs  ዋጋ፦  400ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "26",
         "  Door Bottom Seal Strip Stopper በበርዎ ስር አቧራ እና ነፍሳት እንዳይገባ   የሚከላከል!!!  High Quality  ® አንደኛው ወፍራሙ   Keep Hot and Cold Air isolated   Reduce The Noise Outside and Door slamming   No       ዋጋ፦  400ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "28",
         "  One Fire Table Lamp  በተች የሚሰራ  ቻርጅ የሚደረግ  ለመኝታ ቤት ፣ ለሳሎን ፣ ለ ሆቴሎች እንዲሁም ለባዝ ሩም  ዋጋ፦  1900ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "30",
         "  Vaccuum flask set  የፔርሙዝ ማግ 3 መጠጫ ኩባያዎች ያሉት ለ 12 ሰአታት የሞቀዉን አሙቆ የቀዘቀዘዉን አቀዝቅዞ ሚያስቀምጥ ድንገት ቢወድቅ እንኳን ከማይሰበር ማቴሪያል የተሰራ 500ሚሊ ሊትር የመያዝ አቅም ያለዉ በተለያየ የከለር አማራጭ የቀረበ ለስጦታ የሚሆን የራሱ ማሸጊያና መያዣ ያለዉ  ዋጋ፦  1200 ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "31",
         "ለመላው የእስልምና እምነት ተከታዮች  በሙሉ እንኳን ለኢድ አል አድሀ(ዓረፋ)  በዓል በሰላም አደረሳችሁ   ዛሬ ሱቃችን ሙሉ ቀን ክፍት ነው   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "32",
         "  Stainless steel potato masher   Quality  ዋጋ፦  450 ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "33",
         "  2 in1 Electric Foot grinder & hair remover  የተረከዝ ሞረድ እና የፀጉር ማንሻ በ ቻርጅ የሚሰራ  ዋጋ፦  1000 ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "35",
         " Garlic press Chopper   Kitchen ginger garlic pressQuick grindingSaving you time Manual extrusion and grinding Stainless steel materialFood grade safety materialDurable and easy to clean Curved plastic handleComfortable to use  ዋጋ፦  350 ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "37",
         "  Over Toilet Storage Rack  የቦታ ጥበት ካለብዎ ይሄ ያስፈልግዎታል  Toilet/Kitchen ላይ ሊጠቀሙበት ይችላሉ የንፅህና ዕቃዎችን፣ ኮስሞቲክሶች እና አትክልት/ፍራፍሬዎችን ሊያስቀምጡበት ይችላሉ  ግድግዳ መብሳት አያስፈልግዎትም  ከጠንካራ ብረት እና ፕላስቲክ የተሰራ እርጥበትን የሚቋቋም የማይዝግ  ማያያዣ ሁክ ያለው  ዋጋ፦  1500 ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "40",
         "  Coffee Cold Extraction Cup  Ice Coffee Maker  Portable Cold Brew Bottle Maker Mug  Leak Proof Airtight Lid with Mesh Filter  Ice Coffee Jar Tea Infuser for Outdoor  የአጠቃቀም ዘዴ  1⃣,ለቡና ዱቄት የሚሆን በቂ ቦታ በመተው በውሃ መሙላት ይጀምሩ  2⃣,ውስጥ ባለው ማጣሪያ የሚፈልጉትን የቡና መጠን ይጨምሩ  3⃣,የማጣሪያውን ክዳን አጥብቀው ይዝጉ እና የቡናውን ቦታ በእኩል መጠን ለማከፋፈል ቀስ ብለው ማወዛወዝ  4⃣,ለተወሰኑ ሰአታት ፍሪጅ ውስጥ ማስቀመጥ  5⃣, ቀዝቃዛ ቡናዎን በበረዶ ወይም ከወተት ጋር በመቀላቀል መጠጣት ይችላሉ  ዋጋ፦  900 ብር   ውስን ፍሬ ነው ያለው    አድራሻ   #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "42",
         "   Floodlight Head lamp   rechargeable  light weight   Applicable to various scenarios such as  camping    night running    Repairing    Cycling  ዋጋ፦  950 ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "44",
         "  Car Solar Aromatherapy  በ ፀሀይ ብርሀን የሚሰራ ለመኪናዎ ጥሩ ጠረን የሚያላብስ እየተሽከረከረ አየር በማፅዳት ጥሩ መአዛን የሚሰጥ   ለመኪናዎ ተጨማሪ ውበትን የሚሰጥ   በተመጣጣኝ ዋጋ   ዋጋ፦  850 ብር    ውስን ፍሬ ነው ያለው   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "46",
         "  Silicone Thumb Cutter  የጣት ጥፍር መከላከያ ሽንኩርት ሲልጡ ፣ ዝንጅብል እና በርበሬ  ዘለላ ሲቀነጥሱ ጥፍርን ከጉዳት የሚከላከል Material: Stainless Steel, Silicone, pp colour   ዋጋ፦  250 ብር  ውስን ፍሬ ነው ያለው    አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ],
        [
         "48",
         "   Portable drying rack Clips cloth hanger  ልብስ ማስጫ  የሚዘረጋ የሚሰበሰብ የራሱ የልብስ መቆንጠጫ ያለው 180 cm ርዝመት  ተንቀሳቃሽ  ዋጋ፦  500 ብር  ውስን ፍሬ ነው ያለው   አድራሻ  #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06         0902660722   0928460606    በTelegram ለማዘዝ  ይጠቀሙ       ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን   "
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 30
       }
      },
      "text/plain": [
       "0       Imitation Volcano Humidifier with LED Light ...\n",
       "1        Baby Carrier  በፈለጉት አቅጣጫ ልጅዎን በምቾት ማዘል ያስችል...\n",
       "2       Smart Usb Ultrasonic Car And Home Air Humidi...\n",
       "4       Baby Head Helmet Cotton Walk Safety Hat Brea...\n",
       "6        በረፍት ቀንዎ ሱቅ ላይ መስተናገድ ለምትፈልጉ ውድ ደንበኞቻችን   ነ...\n",
       "7        Baby knee socks  ልጅዎ መዳህ በሚጀምሩበት ጊዜ ተመራጭ  ዋ...\n",
       "9       5-in-1 Trouser Hanger የሱሪ ማስቀመጫ  ዋጋ፦  650 ብር...\n",
       "10      WaterProof Shower Cap    reusable  ዋጋ፦  200 ...\n",
       "11       Shock and Noise Cancelling Washing Machine ...\n",
       "14       Magic Silicone Dish Washing Gloves  High Qu...\n",
       "16       13pc Portable Health Care Kit  Designed For...\n",
       "17       Three-layer Baby Milk Powder Container    H...\n",
       "18       Three-layer Baby Milk Powder Container    H...\n",
       "19     አልቋል ለተባላችሁ በድጋሜ ገብቷል    Only baby 3in1 doubl...\n",
       "21       Mini Pocket UV Umbrella   የቀለም አማራጭ አላቸው  በ...\n",
       "23      Waterproof Baby Urine Mat Cover  ውሃ የማያስገባ እ...\n",
       "24      Space triangles for hangers   12pcs  ዋጋ፦  40...\n",
       "26      Door Bottom Seal Strip Stopper በበርዎ ስር አቧራ እ...\n",
       "28      One Fire Table Lamp  በተች የሚሰራ  ቻርጅ የሚደረግ  ለመ...\n",
       "30      Vaccuum flask set  የፔርሙዝ ማግ 3 መጠጫ ኩባያዎች ያሉት ...\n",
       "31    ለመላው የእስልምና እምነት ተከታዮች  በሙሉ እንኳን ለኢድ አል አድሀ(ዓረ...\n",
       "32      Stainless steel potato masher   Quality  ዋጋ፦...\n",
       "33      2 in1 Electric Foot grinder & hair remover  ...\n",
       "35     Garlic press Chopper   Kitchen ginger garlic ...\n",
       "37      Over Toilet Storage Rack  የቦታ ጥበት ካለብዎ ይሄ ያስ...\n",
       "40      Coffee Cold Extraction Cup  Ice Coffee Maker...\n",
       "42       Floodlight Head lamp   rechargeable  light ...\n",
       "44      Car Solar Aromatherapy  በ ፀሀይ ብርሀን የሚሰራ ለመኪና...\n",
       "46      Silicone Thumb Cutter  የጣት ጥፍር መከላከያ ሽንኩርት ሲ...\n",
       "48       Portable drying rack Clips cloth hanger  ልብ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['message'].drop_duplicates().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "199fff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('../labeled_data.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "with open('../improved_labeled.txt', 'w') as f:\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        f.write(line)\n",
    "        if(i == 5):\n",
    "            f.write('\\n')\n",
    "            i = 0\n",
    "        i+=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e80da03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066 1067\n"
     ]
    }
   ],
   "source": [
    "train_data = 0 \n",
    "test_data = 0\n",
    "all_lines = []\n",
    "with open('../labeled_data.txt', 'r') as f:\n",
    "    all_lines = f.readlines()\n",
    "    total_row = len(all_lines)\n",
    "    #last index of the training data\n",
    "    train_data =  round((total_row * 80)/100)\n",
    "    #start index of the testing data\n",
    "    test_data = train_data+1\n",
    "    print(train_data, test_data)\n",
    "    f.close()\n",
    "with open('../data/train.txt', 'w') as f:\n",
    "    for i in range(0, train_data):\n",
    "        f.write(all_lines[i])\n",
    "    f.close()\n",
    " \n",
    "with open('../data/test.txt', 'w') as f:\n",
    "    for i in range(test_data, len(all_lines)):\n",
    "        f.write(all_lines[i])\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "988e446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Malformed line encountered: መግዛት\n",
      "Warning: Malformed line encountered: ቦታ\n",
      "Warning: Malformed line encountered: (\n",
      "Warning: Malformed line encountered: )\n",
      "Warning: Malformed line encountered: #\n",
      "Warning: Malformed line encountered: 4⃣\n",
      "Warning: Malformed line encountered: ፣\n",
      "Warning: Malformed line encountered: #\n",
      "{'tokens': ['imitation', 'volcano', 'humidifier', 'with', 'led', 'light'], 'ner_tags': ['B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT']}\n",
      "Total sentences read from file: 267\n",
      "Total sentences: 267\n",
      "Train sentences: 213\n",
      "Validation sentences: 27\n",
      "Test sentences: 27\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict,Dataset\n",
    "\n",
    "# Assuming your CoNLL files are named train.txt, validation.txt, test.txt\n",
    "# and are in a directory called 'data'.\n",
    "\n",
    "# If your CoNLL files are directly in a specified path\n",
    "try:\n",
    "    # Use 'conll2003' builder if your format is strictly CoNLL-2003-like\n",
    "    # The 'features' argument might need adjustment based on your exact columns\n",
    "    # For a custom CoNLL file with just 'tokens' and 'ner_tags' as the last column:\n",
    "    # You might need to write a small custom loading script if it's not strictly CoNLL-2003\n",
    "    # which has 4 columns (token, pos, chunk, ner_tag).\n",
    "\n",
    "    # Common scenario: your custom data has two columns: token and NER tag.\n",
    "    # You can adapt the loading or write a simple parser.\n",
    "\n",
    "    # Option A: If your data closely matches CoNLL2003 (token, POS, chunk, NER_tag)\n",
    "    # This is for public datasets following strict CoNLL2003 format\n",
    "    # raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "    # Option B: If your custom CoNLL has only two columns (word, NER_tag)\n",
    "    # You'll likely need a custom loading function.\n",
    "    # Let's write a simple one.\n",
    "\n",
    "    def read_conll_file_all(file_path):\n",
    "        \"\"\"Reads a CoNLL formatted file and returns a list of dictionaries.\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line: # Empty line signals end of a sentence\n",
    "                if tokens:\n",
    "                    data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "                tokens = []\n",
    "                ner_tags = []\n",
    "            elif line.startswith(\"-DOCSTART-\"):\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    ner_tags.append(parts[-1])\n",
    "                else:\n",
    "                    print(f\"Warning: Malformed line encountered: {line}\")\n",
    "        if tokens: # Add the last sentence if file doesn't end with blank line\n",
    "            data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "        return data\n",
    "\n",
    "    # Load your custom CoNLL files\n",
    "    #train_data = read_conll_file(\"../data/train.txt\")\n",
    "    #test_data = read_conll_file(\"../data/test.txt\")\n",
    "    #Load all\n",
    "    # all_data = read_conll_file_all('../labeled_data.txt')\n",
    "    # print(all_data)\n",
    "    # full_dataset = Dataset.from_list(all_data)\n",
    "    # print(full_dataset)\n",
    "    # train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42, train_size=0.8)\n",
    "\n",
    "    # #test_val_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "    # raw_datasets = DatasetDict({\n",
    "    #     \"train\": train_test_split['train'],\n",
    "    #     \"validation\": train_test_split['train'],\n",
    "    #     \"test\": train_test_split['test'],\n",
    "    # })\n",
    "    # print(f\"Total sentences: {len(full_dataset)}\")\n",
    "    # print(f\"Train sentences: {len(raw_datasets['train'])}\")\n",
    "    # print(f\"Validation sentences: {len(raw_datasets['validation'])}\")\n",
    "    # print(f\"Test sentences: {len(raw_datasets['test'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load CoNLL data directly using load_dataset or custom parser: {e}\")\n",
    "    print(\"Please ensure your data files are correctly placed and formatted.\")\n",
    "    # Fallback or error handling\n",
    "    raw_datasets = None # Or raise an error\n",
    "    # Let's assume your combined CoNLL data is in 'combined_ner_data.txt'\n",
    "# Or, you can manually combine your manually created CoNLL files into one first.\n",
    "combined_data = read_conll_file_all(\"../improved_labeled.txt\")\n",
    "print(combined_data[0])\n",
    "# Convert to Hugging Face Dataset\n",
    "full_dataset = Dataset.from_list(combined_data)\n",
    "# Convert \n",
    "\n",
    "# Check the number of samples BEFORE splitting\n",
    "num_samples = len(full_dataset)\n",
    "print(f\"Total sentences read from file: {num_samples}\")\n",
    "\n",
    "if num_samples < 2:\n",
    "    raise ValueError(\n",
    "        \"Your combined_ner_data.txt contains less than 2 sentences. \"\n",
    "        \"You need at least 2 sentences to perform a train-test split. \"\n",
    "        \"Please add more data to your CoNLL file.\"\n",
    "    )\n",
    "elif num_samples < 3 and (num_samples * 0.3 >= 1 or num_samples * 0.5 >= 1):\n",
    "     print(\"Warning: Very few samples. Splitting ratios might result in very small sets. \"\n",
    "           \"Consider using integer `test_size` values if specific counts are desired.\")\n",
    "# --- Step 2: Split the dataset into train, validation, and test ---\n",
    "# First, split into train and temp (validation + test)\n",
    "train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42) # 70% train, 30% temp\n",
    "\n",
    "# Then, split the temp set into validation and test\n",
    "test_validation_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42) # 50% of 30% for test, 50% for val\n",
    "\n",
    "# Create the final DatasetDict\n",
    "splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': test_validation_split['train'], # Using 'train' from the second split as validation\n",
    "    'test': test_validation_split['test']\n",
    "})\n",
    "\n",
    "print(f\"Total sentences: {len(full_dataset)}\")\n",
    "print(f\"Train sentences: {len(splits['train'])}\")\n",
    "print(f\"Validation sentences: {len(splits['validation'])}\")\n",
    "print(f\"Test sentences: {len(splits['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def write_conll_file(data_list, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in data_list:\n",
    "            for token, tag in zip(example[\"tokens\"], example[\"ner_tags\"]):\n",
    "                f.write(f\"{token}\\t{tag}\\n\")\n",
    "            f.write(\"\\n\") # Blank line to separate sentences\n",
    "output_dir = \"../data/data_splits\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "write_conll_file(splits['train'], os.path.join(output_dir, \"train.txt\"))\n",
    "write_conll_file(splits['validation'], os.path.join(output_dir, \"validation.txt\"))\n",
    "write_conll_file(splits['test'], os.path.join(output_dir, \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfb72cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Labels: ['B-LOC', 'B-PRICE', 'B-PRODUCT', 'I-LOC', 'I-PRICE', 'I-PRODUCT', 'O']\n",
      "Label to ID Mapping: {'B-LOC': 0, 'B-PRICE': 1, 'B-PRODUCT': 2, 'I-LOC': 3, 'I-PRICE': 4, 'I-PRODUCT': 5, 'O': 6}\n"
     ]
    }
   ],
   "source": [
    "# Collect all unique NER tags from your dataset\n",
    "unique_tags = set()\n",
    "for dataset_split in raw_datasets.values():\n",
    "    for example in dataset_split:\n",
    "        unique_tags.update(example[\"ner_tags\"])\n",
    "\n",
    "# Sort them for consistent ID assignment\n",
    "label_list = sorted(list(unique_tags))\n",
    "# Make sure \"O\" is always at ID 0 if preferred, or handle BIOES for correct metric calculation.\n",
    "# For simplicity, if your tags are already BIO/BIOES, sorting is usually fine.\n",
    "# If you need to add specific tags, e.g., for missing 'I-' tags, ensure they are present.\n",
    "\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "print(f\"Detected Labels: {label_list}\")\n",
    "print(f\"Label to ID Mapping: {label_to_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a9ea7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.21 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 66.41 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "{'input_ids': [0, 97751, 1363, 2741, 38938, 165847, 85789, 678, 12441, 22729, 728, 37830, 10824, 110020, 146329, 2627, 31415, 2237, 50452, 85863, 2370, 6759, 13075, 548, 2627, 48014, 80667, 54164, 59404, 35648, 6, 100579, 548, 29831, 13199, 16863, 548, 140042, 2370, 60014, 9171, 454, 93561, 47227, 454, 4555, 191973, 454, 13357, 2202, 454, 115742, 454, 31531, 5653, 137526, 47885, 91, 8194, 64, 7, 9016, 3894, 9550, 89709, 191539, 3894, 3882, 158332, 161025, 728, 75809, 25561, 13253, 9039, 7872, 2934, 169422, 91602, 16333, 124449, 623, 17680, 10824, 70317, 816, 24163, 29307, 23374, 15546, 2258, 25388, 728, 204281, 1437, 175386, 49648, 152806, 62696, 237390, 1437, 4363, 9039, 2202, 30936, 69870, 29597, 54667, 80667, 54164, 149678, 35648, 6, 100579, 548, 29831, 13199, 16863, 548, 140042, 468, 2370, 60014, 9171, 454, 93561, 47227, 454, 4555, 191973, 454, 13357, 2202, 454, 115742, 454, 31531, 5653, 137526, 47885, 91, 8194, 64, 7, 9016, 3894, 9550, 89709, 191539, 3894, 3882, 158332, 161025, 728, 75809, 25561, 13253, 9039, 7872, 2934, 169422, 91602, 16333, 124449, 243084, 623, 17680, 10824, 70317, 816, 24163, 29307, 23374, 18775, 1821, 275, 19914, 1681, 1771, 2258, 136, 5368, 1831, 165847, 85789, 678, 10576, 7844, 12441, 22729, 7311, 11192, 9, 161789, 728, 37830, 10824, 110020, 146329, 2627, 31415, 2237, 50452, 946, 2237, 126399, 85863, 2370, 6759, 13075, 548, 2627, 48014, 57849, 67, 70, 47506, 17366, 28032, 935, 38043, 64881, 90, 678, 903, 47823, 50997, 6, 45486, 1831, 6, 6133, 165847, 85789, 80667, 54164, 76094, 35648, 6, 100579, 548, 29831, 13199, 16863, 548, 140042, 2370, 60014, 9171, 454, 93561, 47227, 454, 4555, 191973, 454, 13357, 2202, 454, 115742, 454, 31531, 5653, 137526, 47885, 91, 8194, 64, 7, 9016, 3894, 9550, 89709, 191539, 3894, 3882, 158332, 161025, 728, 75809, 25561, 13253, 9039, 7872, 2934, 169422, 91602, 16333, 124449, 243084, 623, 17680, 10824, 70317, 816, 24163, 29307, 23374, 18775, 1821, 275, 19914, 1681, 1771, 2258, 136, 5368, 1831, 165847, 85789, 678, 10576, 7844, 12441, 22729, 7311, 11192, 9, 161789, 728, 37830, 10824, 110020, 146329, 2627, 31415, 2237, 50452, 946, 2237, 126399, 85863, 2370, 6759, 13075, 548, 2627, 48014, 57849, 67, 70, 47506, 17366, 28032, 935, 38043, 64881, 90, 678, 903, 47823, 50997, 6, 45486, 1831, 6, 6133, 165847, 85789, 80667, 54164, 76094, 35648, 6, 100579, 548, 29831, 13199, 16863, 548, 101848, 3851, 2370, 60014, 9171, 454, 93561, 47227, 454, 4555, 191973, 454, 13357, 2202, 454, 115742, 454, 31531, 5653, 137526, 47885, 91, 8194, 64, 7, 9016, 3894, 9550, 89709, 191539, 3894, 3882, 158332, 161025, 728, 75809, 25561, 13253, 9039, 7872, 2934, 169422, 91602, 16333, 124449, 243084, 623, 17680, 10824, 70317, 816, 24163, 29307, 23374, 15546, 10336, 71464, 126, 235489, 35691, 81900, 1256, 71191, 2886, 10336, 429, 147, 47, 4028, 603, 2874, 9146, 7922, 903, 12996, 83, 7228, 111, 351, 9, 74735, 235489, 4912, 442, 831, 186, 126596, 2886, 47, 11177, 7, 756, 70, 12921, 13267, 111, 10336, 70, 12996, 83, 2843, 71191, 2886, 136, 70, 15546, 242, 7, 10336, 1221, 959, 186, 2906, 15123, 136, 842, 67192, 3229, 75169, 80667, 54164, 45334, 35648, 6, 100579, 548, 29831, 13199, 16863, 548, 140042, 2370, 60014, 9171, 454, 93561, 47227, 454, 4555, 191973, 454, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 2, 5, 5, 5, 5, 5, 5, 5, 5, 6, -100, -100, -100, -100, 6, -100, 6, -100, 6, 6, -100, -100, -100, 6, -100, 1, 4, 4, 4, 6, -100, -100, 6, -100, 6, -100, 6, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, -100, -100, -100, 6, -100, -100, -100, 6, -100, -100, 6, -100, -100, 6, -100, 6, -100, -100, 6, -100, -100, -100, -100, 6, -100, -100, 2, 5, 5, 6, -100, -100, 6, 6, -100, 6, -100, -100, 6, -100, -100, 6, -100, -100, -100, 1, 4, 4, 4, 6, -100, -100, 6, -100, 6, -100, 6, 6, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, -100, -100, -100, 6, -100, -100, -100, 6, -100, -100, 6, -100, -100, 6, -100, 6, -100, -100, 6, 6, -100, -100, -100, -100, 6, -100, -100, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, -100, -100, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, -100, 6, 6, 6, 6, 6, 6, 6, -100, 6, 6, 6, 2, 5, 5, 5, 5, 5, 5, 5, 1, 4, 4, 4, 6, -100, -100, 6, -100, 6, -100, 6, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, -100, -100, -100, 6, -100, -100, -100, 6, -100, -100, 6, -100, -100, 6, -100, 6, -100, -100, 6, 6, -100, -100, -100, -100, 6, -100, -100, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, -100, -100, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, -100, 6, 6, 6, 6, 6, 6, 6, -100, 6, 6, 6, 2, 5, 5, 5, 5, 5, 5, 5, 1, 4, 4, 4, 6, -100, -100, 6, -100, 6, -100, 6, -100, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, -100, -100, -100, 6, -100, -100, -100, 6, -100, -100, 6, -100, -100, 6, -100, 6, -100, -100, 6, 6, -100, -100, -100, -100, 6, -100, -100, 2, 5, 5, 5, 5, 5, 5, 5, 6, -100, 6, -100, -100, 6, -100, -100, 6, -100, 6, 6, 6, 6, 6, 6, 6, -100, -100, 6, 6, 6, 6, 6, 6, -100, 6, 6, -100, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, -100, 6, 6, 6, 6, -100, 6, 6, 6, 6, 6, -100, 6, 6, -100, 6, 6, 1, 4, 4, 4, 6, -100, -100, 6, -100, 6, -100, 6, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, -100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"xlm-roberta-base\" # or xlm-roberta-large\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None: # Special tokens\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx: # First token of a new word\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            else: # Subsequent token of the same word\n",
    "                current_tag = label[word_idx]\n",
    "                # If original tag was B-X, make subsequent I-X. If I-X, keep I-X. Else -100.\n",
    "                if current_tag.startswith(\"B-\"):\n",
    "                    label_ids.append(label_to_id[\"I-\" + current_tag[2:]])\n",
    "                elif current_tag.startswith(\"I-\"):\n",
    "                    label_ids.append(label_to_id[current_tag])\n",
    "                else: # O tag or other types\n",
    "                    label_ids.append(-100) # Or keep O if you want, but -100 is safer for loss\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization and alignment to your loaded datasets\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True, # Process in batches for speed\n",
    "    remove_columns=raw_datasets[\"train\"].column_names, # Remove original 'tokens' and 'ner_tags'\n",
    ")\n",
    "\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets[\"train\"][0]) # Check a sample to ensure it looks correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b23c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load the model with the correct number of labels\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label, # Pass your label mappings\n",
    "    label2id=label_to_id, # Pass your label mappings\n",
    ")\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (where label is -100)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm_roberta_ner_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set\n",
    "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(results)\n",
    "\n",
    "# Optional: Get a full classification report on the test set\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_predictions = [\n",
    "    [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [id_to_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "print(\"\\n--- Final Classification Report on Test Set ---\")\n",
    "print(classification_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4adaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison loop example\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "from datasets import DatasetDict\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "# Assume raw_datasets, label_to_id, id_to_label, read_conll_file, compute_metrics are defined from previous steps\n",
    "\n",
    "# List of models to compare\n",
    "model_names_to_compare = [\n",
    "    \"xlm-roberta-base\",\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    \"bert-base-multilingual-cased\"\n",
    "]\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "for model_name in model_names_to_compare:\n",
    "    print(f\"\\n--- Fine-tuning {model_name} ---\")\n",
    "\n",
    "    try:\n",
    "        # Load tokenizer specific to the model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Apply tokenization and alignment\n",
    "        # Need to re-apply map because tokenizer changes\n",
    "        def tokenize_and_align_labels(examples):\n",
    "            tokenized_inputs = tokenizer(\n",
    "                examples[\"tokens\"],\n",
    "                truncation=True,\n",
    "                is_split_into_words=True,\n",
    "            )\n",
    "            labels = []\n",
    "            for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "                previous_word_idx = None\n",
    "                label_ids = []\n",
    "                for word_idx in word_ids:\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:\n",
    "                        label_ids.append(label_to_id[label[word_idx]])\n",
    "                    else:\n",
    "                        current_tag = label[word_idx]\n",
    "                        if current_tag.startswith(\"B-\"):\n",
    "                            label_ids.append(label_to_id[\"I-\" + current_tag[2:]])\n",
    "                        elif current_tag.startswith(\"I-\"):\n",
    "                            label_ids.append(label_to_id[current_tag])\n",
    "                        else:\n",
    "                            label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                labels.append(label_ids)\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=raw_datasets[\"train\"].column_names,\n",
    "            load_from_cache_file=False # Important to re-process for each tokenizer\n",
    "        )\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(label_list),\n",
    "            id2label=id_to_label,\n",
    "            label2id=label_to_id,\n",
    "        )\n",
    "\n",
    "        # Data collator (tokenizer needed for padding)\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "        # Training arguments (adjust output_dir for each model)\n",
    "        output_dir = f\"drive/MyDrive/model_data/model/results_{model_name.replace('/', '_')}_ner\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3, # Keep consistent for comparison\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=f'drive/MyDrive/model_data/model/logs_{model_name.replace(\"/\", \"_\")}',\n",
    "            logging_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            report_to=\"tensorboard\",\n",
    "            # disable_tqdm=True, # Optional: disable progress bars for cleaner logs if running many\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics, # Re-use the seqeval compute_metrics\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate on validation set (best model will be loaded)\n",
    "        val_metrics = trainer.evaluate(tokenized_datasets[\"validation\"])\n",
    "        print(f\"Validation metrics for {model_name}: {val_metrics}\")\n",
    "\n",
    "        # Store results\n",
    "        results_summary[model_name] = {\n",
    "            \"validation_f1\": val_metrics.get('eval_f1'),\n",
    "            \"validation_precision\": val_metrics.get('eval_precision'),\n",
    "            \"validation_recall\": val_metrics.get('eval_recall'),\n",
    "            \"validation_loss\": val_metrics.get('eval_loss')\n",
    "            # \"training_time_seconds\": trainer.state.global_step * training_args.logging_steps * (trainer.state.log_history[-1]['loss'] / trainer.state.log_history[-1]['learning_rate']) if trainer.state.log_history else 'N/A' # A rough estimate\n",
    "        }\n",
    "\n",
    "        # Save the best model explicitly (Trainer might already do it, but good to be sure)\n",
    "        # trainer.save_model(os.path.join(output_dir, \"best_model\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fine-tuning {model_name}: {e}\")\n",
    "        results_summary[model_name] = {\"error\": str(e)}\n",
    "\n",
    "print(\"\\n--- Comparison Summary ---\")\n",
    "for model, metrics in results_summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd85d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm-roberta-base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
