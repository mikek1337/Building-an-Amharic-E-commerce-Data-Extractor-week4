{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d931a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Malformed line encountered: መግዛት\n",
      "Warning: Malformed line encountered: ቦታ\n",
      "Warning: Malformed line encountered: (\n",
      "Warning: Malformed line encountered: )\n",
      "Warning: Malformed line encountered: #\n",
      "Warning: Malformed line encountered: 4⃣\n",
      "Warning: Malformed line encountered: ፣\n",
      "Warning: Malformed line encountered: #\n",
      "{'tokens': ['imitation', 'volcano', 'humidifier', 'with', 'led', 'light'], 'ner_tags': ['B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT']}\n",
      "Total sentences read from file: 267\n",
      "Total sentences: 267\n",
      "Train sentences: 213\n",
      "Validation sentences: 27\n",
      "Test sentences: 27\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict,Dataset\n",
    "\n",
    "# Assuming your CoNLL files are named train.txt, validation.txt, test.txt\n",
    "# and are in a directory called 'data'.\n",
    "\n",
    "# If your CoNLL files are directly in a specified path\n",
    "try:\n",
    "    # Use 'conll2003' builder if your format is strictly CoNLL-2003-like\n",
    "    # The 'features' argument might need adjustment based on your exact columns\n",
    "    # For a custom CoNLL file with just 'tokens' and 'ner_tags' as the last column:\n",
    "    # You might need to write a small custom loading script if it's not strictly CoNLL-2003\n",
    "    # which has 4 columns (token, pos, chunk, ner_tag).\n",
    "\n",
    "    # Common scenario: your custom data has two columns: token and NER tag.\n",
    "    # You can adapt the loading or write a simple parser.\n",
    "\n",
    "    # Option A: If your data closely matches CoNLL2003 (token, POS, chunk, NER_tag)\n",
    "    # This is for public datasets following strict CoNLL2003 format\n",
    "    # raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "    # Option B: If your custom CoNLL has only two columns (word, NER_tag)\n",
    "    # You'll likely need a custom loading function.\n",
    "    # Let's write a simple one.\n",
    "\n",
    "    def read_conll_file_all(file_path):\n",
    "        \"\"\"Reads a CoNLL formatted file and returns a list of dictionaries.\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line: # Empty line signals end of a sentence\n",
    "                if tokens:\n",
    "                    data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "                tokens = []\n",
    "                ner_tags = []\n",
    "            elif line.startswith(\"-DOCSTART-\"):\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    ner_tags.append(parts[-1])\n",
    "                else:\n",
    "                    print(f\"Warning: Malformed line encountered: {line}\")\n",
    "        if tokens: # Add the last sentence if file doesn't end with blank line\n",
    "            data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "        return data\n",
    "\n",
    "    # Load your custom CoNLL files\n",
    "    #train_data = read_conll_file(\"../data/train.txt\")\n",
    "    #test_data = read_conll_file(\"../data/test.txt\")\n",
    "    #Load all\n",
    "    # all_data = read_conll_file_all('../labeled_data.txt')\n",
    "    # print(all_data)\n",
    "    # full_dataset = Dataset.from_list(all_data)\n",
    "    # print(full_dataset)\n",
    "    # train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42, train_size=0.8)\n",
    "\n",
    "    # #test_val_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "    # raw_datasets = DatasetDict({\n",
    "    #     \"train\": train_test_split['train'],\n",
    "    #     \"validation\": train_test_split['train'],\n",
    "    #     \"test\": train_test_split['test'],\n",
    "    # })\n",
    "    # print(f\"Total sentences: {len(full_dataset)}\")\n",
    "    # print(f\"Train sentences: {len(raw_datasets['train'])}\")\n",
    "    # print(f\"Validation sentences: {len(raw_datasets['validation'])}\")\n",
    "    # print(f\"Test sentences: {len(raw_datasets['test'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load CoNLL data directly using load_dataset or custom parser: {e}\")\n",
    "    print(\"Please ensure your data files are correctly placed and formatted.\")\n",
    "    # Fallback or error handling\n",
    "    raw_datasets = None # Or raise an error\n",
    "    # Let's assume your combined CoNLL data is in 'combined_ner_data.txt'\n",
    "# Or, you can manually combine your manually created CoNLL files into one first.\n",
    "combined_data = read_conll_file_all(\"../data/improved_labeled.txt\")\n",
    "print(combined_data[0])\n",
    "# Convert to Hugging Face Dataset\n",
    "full_dataset = Dataset.from_list(combined_data)\n",
    "# Convert \n",
    "\n",
    "# Check the number of samples BEFORE splitting\n",
    "num_samples = len(full_dataset)\n",
    "print(f\"Total sentences read from file: {num_samples}\")\n",
    "\n",
    "if num_samples < 2:\n",
    "    raise ValueError(\n",
    "        \"Your combined_ner_data.txt contains less than 2 sentences. \"\n",
    "        \"You need at least 2 sentences to perform a train-test split. \"\n",
    "        \"Please add more data to your CoNLL file.\"\n",
    "    )\n",
    "elif num_samples < 3 and (num_samples * 0.3 >= 1 or num_samples * 0.5 >= 1):\n",
    "     print(\"Warning: Very few samples. Splitting ratios might result in very small sets. \"\n",
    "           \"Consider using integer `test_size` values if specific counts are desired.\")\n",
    "# --- Step 2: Split the dataset into train, validation, and test ---\n",
    "# First, split into train and temp (validation + test)\n",
    "train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42) # 70% train, 30% temp\n",
    "\n",
    "# Then, split the temp set into validation and test\n",
    "test_validation_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42) # 50% of 30% for test, 50% for val\n",
    "\n",
    "# Create the final DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': test_validation_split['train'], # Using 'train' from the second split as validation\n",
    "    'test': test_validation_split['test']\n",
    "})\n",
    "\n",
    "print(f\"Total sentences: {len(full_dataset)}\")\n",
    "print(f\"Train sentences: {len(raw_datasets['train'])}\")\n",
    "print(f\"Validation sentences: {len(raw_datasets['validation'])}\")\n",
    "print(f\"Test sentences: {len(raw_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7748529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Labels: ['B-LOC', 'B-PRICE', 'B-PRODUCT', 'I-LOC', 'I-PRICE', 'I-PRODUCT', 'O']\n",
      "Label to ID Mapping: {'B-LOC': 0, 'B-PRICE': 1, 'B-PRODUCT': 2, 'I-LOC': 3, 'I-PRICE': 4, 'I-PRODUCT': 5, 'O': 6}\n"
     ]
    }
   ],
   "source": [
    "# Collect all unique NER tags from your dataset\n",
    "unique_tags = set()\n",
    "for dataset_split in raw_datasets.values():\n",
    "    for example in dataset_split:\n",
    "        unique_tags.update(example[\"ner_tags\"])\n",
    "\n",
    "# Sort them for consistent ID assignment\n",
    "label_list = sorted(list(unique_tags))\n",
    "# Make sure \"O\" is always at ID 0 if preferred, or handle BIOES for correct metric calculation.\n",
    "# For simplicity, if your tags are already BIO/BIOES, sorting is usually fine.\n",
    "# If you need to add specific tags, e.g., for missing 'I-' tags, ensure they are present.\n",
    "\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "print(f\"Detected Labels: {label_list}\")\n",
    "print(f\"Label to ID Mapping: {label_to_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"xlm-roberta-base\" # or xlm-roberta-large\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None: # Special tokens\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx: # First token of a new word\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            else: # Subsequent token of the same word\n",
    "                current_tag = label[word_idx]\n",
    "                # If original tag was B-X, make subsequent I-X. If I-X, keep I-X. Else -100.\n",
    "                if current_tag.startswith(\"B-\"):\n",
    "                    label_ids.append(label_to_id[\"I-\" + current_tag[2:]])\n",
    "                elif current_tag.startswith(\"I-\"):\n",
    "                    label_ids.append(label_to_id[current_tag])\n",
    "                else: # O tag or other types\n",
    "                    label_ids.append(-100) # Or keep O if you want, but -100 is safer for loss\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization and alignment to your loaded datasets\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True, # Process in batches for speed\n",
    "    remove_columns=raw_datasets[\"train\"].column_names, # Remove original 'tokens' and 'ner_tags'\n",
    ")\n",
    "\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets[\"train\"][0]) # Check a sample to ensure it looks correct   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0f075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (where label is -100)\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id_to_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188098ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_to_compare = [\n",
    "    \"xlm-roberta-base\",\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    \"bert-base-multilingual-cased\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison loop example\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "from datasets import DatasetDict\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "# Assume raw_datasets, label_to_id, id_to_label, read_conll_file, compute_metrics are defined from previous steps\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "for model_name in model_names_to_compare:\n",
    "    print(f\"\\n--- Fine-tuning {model_name} ---\")\n",
    "\n",
    "    try:\n",
    "        # Load tokenizer specific to the model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Apply tokenization and alignment\n",
    "        # Need to re-apply map because tokenizer changes\n",
    "        def tokenize_and_align_labels(examples):\n",
    "            tokenized_inputs = tokenizer(\n",
    "                examples[\"tokens\"],\n",
    "                truncation=True,\n",
    "                is_split_into_words=True,\n",
    "            )\n",
    "            labels = []\n",
    "            for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "                previous_word_idx = None\n",
    "                label_ids = []\n",
    "                for word_idx in word_ids:\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:\n",
    "                        label_ids.append(label_to_id[label[word_idx]])\n",
    "                    else:\n",
    "                        current_tag = label[word_idx]\n",
    "                        if current_tag.startswith(\"B-\"):\n",
    "                            label_ids.append(label_to_id[\"I-\" + current_tag[2:]])\n",
    "                        elif current_tag.startswith(\"I-\"):\n",
    "                            label_ids.append(label_to_id[current_tag])\n",
    "                        else:\n",
    "                            label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                labels.append(label_ids)\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=raw_datasets[\"train\"].column_names,\n",
    "            load_from_cache_file=False # Important to re-process for each tokenizer\n",
    "        )\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(label_list),\n",
    "            id2label=id_to_label,\n",
    "            label2id=label_to_id,\n",
    "        )\n",
    "\n",
    "        # Data collator (tokenizer needed for padding)\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "        # Training arguments (adjust output_dir for each model)\n",
    "        output_dir = f\"drive/MyDrive/model_data/model/results_{model_name.replace('/', '_')}_ner\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3, # Keep consistent for comparison\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=f'drive/MyDrive/model_data/model/logs_{model_name.replace(\"/\", \"_\")}',\n",
    "            logging_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            report_to=\"tensorboard\",\n",
    "            # disable_tqdm=True, # Optional: disable progress bars for cleaner logs if running many\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics, # Re-use the seqeval compute_metrics\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate on validation set (best model will be loaded)\n",
    "        val_metrics = trainer.evaluate(tokenized_datasets[\"validation\"])\n",
    "        print(f\"Validation metrics for {model_name}: {val_metrics}\")\n",
    "\n",
    "        # Store results\n",
    "        results_summary[model_name] = {\n",
    "            \"validation_f1\": val_metrics.get('eval_f1'),\n",
    "            \"validation_precision\": val_metrics.get('eval_precision'),\n",
    "            \"validation_recall\": val_metrics.get('eval_recall'),\n",
    "            \"validation_loss\": val_metrics.get('eval_loss')\n",
    "            # \"training_time_seconds\": trainer.state.global_step * training_args.logging_steps * (trainer.state.log_history[-1]['loss'] / trainer.state.log_history[-1]['learning_rate']) if trainer.state.log_history else 'N/A' # A rough estimate\n",
    "        }\n",
    "\n",
    "        # Save the best model explicitly (Trainer might already do it, but good to be sure)\n",
    "        trainer.save_model(f'drive/MyDrive/model_data/model/{model_name}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fine-tuning {model_name}: {e}\")\n",
    "        results_summary[model_name] = {\"error\": str(e)}\n",
    "\n",
    "print(\"\\n--- Comparison Summary ---\")\n",
    "for model, metrics in results_summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfcb167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for model_name in model_names_to_compare:\n",
    "  final_best_model_path = f\"drive/MyDrive/model_data/model/results_{model_name.replace('/', '_')}_ner/checkpoint-14\"\n",
    "  if model_name == \"xlm-roberta-base\":\n",
    "    final_best_model_path = f\"drive/MyDrive/model_data/model/xlm_roberta_ner_results/checkpoint-42\"\n",
    "  print(final_best_model_path)\n",
    "  loaded_tokenizer = AutoTokenizer.from_pretrained(final_best_model_path)\n",
    "  loaded_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    final_best_model_path,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    "  )\n",
    "  loaded_model.to(device)\n",
    "  ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=loaded_model,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    aggregation_strategy=\"simple\", # Optional: aggregates subword tokens into full words\n",
    "    device=0 if torch.cuda.is_available() else -1 # 0 for GPU, -1 for CPU\n",
    "  )\n",
    "  amharic_text = \"Smart Usb Ultrasonic Car And Home Air Humidifier With Colorful Led Light Original High-quality በኤሌክትሪክ የሚሰራ ለቤትና ለመኪና መልካም መዓዛን የሚሰጥ  Elevate the comfort level within your living premises with this fantastic Green Lion Air Mist Humidifier ዋጋ፦ 1100 ብር ውስን ፍሬ ነው ያለን አድራሻ #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06 0902660722 0928460606 በTelegram ለማዘዝ  ይጠቀሙ ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን\"\n",
    "  print(f\"\\nInference on text 1: '{amharic_text}'{model_name}\")\n",
    "  results_1 = ner_pipeline(amharic_text)\n",
    "  print(results_1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed83d8",
   "metadata": {},
   "source": [
    "#### Using LIME to analysis token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b825c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LIME Prediction Function Wrapper for a Specific Token ---\n",
    "# Similar to SHAP, but LIME expects predict_proba to return (num_samples, num_classes)\n",
    "# where num_classes are the classes for the target token's prediction.\n",
    "choosen_model = f\"drive/MyDrive/model_data/model/results_distilbert-base-multilingual-cased_ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_best_model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    final_best_model_path,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval() # Set to evaluation mode\n",
    "print(\"Model and tokenizer loaded successfully.\")\n",
    "def get_lime_prediction_wrapper(model, tokenizer, target_word_index_in_original_text):\n",
    "    def predict_proba_fn(texts):\n",
    "        all_target_token_probs = []\n",
    "        for text in texts:\n",
    "            tokenized_input = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                is_split_into_words=False\n",
    "            )\n",
    "            input_ids = tokenized_input[\"input_ids\"].to(model.device)\n",
    "\n",
    "            word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "\n",
    "            target_token_idx = None\n",
    "            for i, wid in enumerate(word_ids):\n",
    "                if wid == target_word_index_in_original_text:\n",
    "                    target_token_idx = i\n",
    "                    break\n",
    "\n",
    "            if target_token_idx is None:\n",
    "                # If target word is missing in perturbed text, return uniform distribution\n",
    "                all_target_token_probs.append(np.full(len(label_list), 1.0 / len(label_list)))\n",
    "                continue\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids)\n",
    "\n",
    "            target_token_logits = outputs.logits.squeeze(0)[target_token_idx, :]\n",
    "            target_token_probs = F.softmax(target_token_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            all_target_token_probs.append(target_token_probs)\n",
    "\n",
    "        return np.array(all_target_token_probs)\n",
    "\n",
    "    return predict_proba_fn\n",
    "\n",
    "# --- LIME Usage Example ---\n",
    "text_to_explain_lime = \"Smart Usb Ultrasonic Car And Home Air Humidifier With Colorful Led Light Original High-quality በኤሌክትሪክ የሚሰራ ለቤትና ለመኪና መልካም መዓዛን የሚሰጥ  Elevate the comfort level within your living premises with this fantastic Green Lion Air Mist Humidifier ዋጋ፦ 1100 ብር ውስን ፍሬ ነው ያለን አድራሻ #መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ቢሮ ቁ S05/S06 0902660722 0928460606 በTelegram ለማዘዝ  ይጠቀሙ ለተጨማሪ ማብራሪያ የቴሌግራም ገፃችን\"\n",
    "# \"Commercial Bank of Ethiopia opened a new branch in 2024.\"\n",
    "original_words_lime = custom_word_tokenizer(text_to_explain_lime)\n",
    "\n",
    "# Let's explain why \"ባንክ\" (Bank) at index 2 is predicted as I-ORG (assuming it is part of an ORG entity).\n",
    "target_word_index_lime = 1 # Index of \"ባንክ\"\n",
    "\n",
    "# Get the LIME prediction wrapper function for this specific token\n",
    "lime_predict_proba_fn = get_lime_prediction_wrapper(model, tokenizer, target_word_index_lime)\n",
    "\n",
    "# Create a LIME explainer\n",
    "explainer_lime = LimeTextExplainer(\n",
    "    class_names=label_list,\n",
    "    split_expression=r'\\s+' # Use regex for spaces to handle multiple spaces\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerating LIME Explanations for word '{original_words_lime[target_word_index_lime]}' in sentence: '{text_to_explain_lime}' (This may take a while)...\")\n",
    "\n",
    "# Explain the instance\n",
    "# `num_features` is the number of words to show in the explanation\n",
    "# `num_samples` is how many perturbed samples LIME creates\n",
    "# `top_labels` is how many of the top predicted labels for the target token to explain\n",
    "explanation_lime = explainer_lime.explain_instance(\n",
    "    text_to_explain_lime,\n",
    "    lime_predict_proba_fn,\n",
    "    num_features=5, # Number of words to highlight\n",
    "    num_samples=1000, # Number of perturbations (can be slow)\n",
    "    top_labels=3 # Show explanations for top 3 predicted labels of the target token\n",
    ")\n",
    "\n",
    "# --- Visualize LIME Explanation ---\n",
    "print(f\"\\nLIME Explanation for '{original_words_lime[target_word_index_lime]}' (index {target_word_index_lime}):\")\n",
    "print(explanation_lime.top_labels)\n",
    "for label_id in explanation_lime.top_labels:\n",
    "    label_name = id_to_label[label_id]\n",
    "    # print(f\"  Explaining for label: {label_name} (Probability: {prob:.4f})\")\n",
    "    print(\"  Word contributions:\")\n",
    "    for word, weight in explanation_lime.as_list(label=label_id):\n",
    "        print(f\"    - '{word}': {weight:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
